# –õ–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–∞—è —Ä–∞–±–æ—Ç–∞ ‚Ññ3 ‚Äî –¢–µ–∫—Å—Ç—ã –∏ —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤

**–¶–µ–ª—å:** –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç, –∞–∫–∫—É—Ä–∞—Ç–Ω–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, –ø–æ—Å—á–∏—Ç–∞—Ç—å —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –∏ –≤—ã–≤–µ—Å—Ç–∏ —Ç–æ–ø-N.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
src/
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ text.py           # –ú–æ–¥—É–ª—å —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º
‚îî‚îÄ‚îÄ lab03/
    ‚îú‚îÄ‚îÄ text_stats.py      # –°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ stdin
    ‚îî‚îÄ‚îÄ README.md          # –≠—Ç–æ—Ç —Ñ–∞–π–ª
```

## –ó–∞–¥–∞–Ω–∏–µ A ‚Äî –ú–æ–¥—É–ª—å `src/lib/text.py`
![–¢–µ—Å—Ç text_stats.py](../../img/lab03/A_1f.png)
![–¢–µ—Å—Ç text_stats.py](../../img/lab03/A_2f.png)
![–¢–µ—Å—Ç text_stats.py](../../img/lab03/A_3f.png)
–ú–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —á–µ—Ç—ã—Ä–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º:

### 1. `normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str`

–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç:
- –ü—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É —Å –ø–æ–º–æ—â—å—é `casefold()` (–ª—É—á—à–µ —á–µ–º `lower()` –¥–ª—è Unicode)
- –ó–∞–º–µ–Ω—è–µ—Ç –≤—Å–µ `—ë/–Å` –Ω–∞ `–µ/–ï`
- –ó–∞–º–µ–Ω—è–µ—Ç —É–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã (`\t`, `\r`, `\n`) –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
- –°—Ö–ª–æ–ø—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –æ–¥–∏–Ω
- –û–±—Ä–µ–∑–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã —Å –∫—Ä–∞—ë–≤

**–ü—Ä–∏–º–µ—Ä—ã:**
```python
normalize("–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t")           # ‚Üí "–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"
normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")              # ‚Üí "–µ–∂–∏–∫, –µ–ª–∫–∞"
normalize("Hello\r\nWorld")          # ‚Üí "hello world"
normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")   # ‚Üí "–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"
```

### 2. `tokenize(text: str) -> list[str]`

–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–æ–∫–µ–Ω—ã (—Å–ª–æ–≤–∞). –¢–æ–∫–µ–Ω ‚Äî —ç—Ç–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤ `\w+` (–±—É–∫–≤—ã/—Ü–∏—Ñ—Ä—ã/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ) —Å –≤–æ–∑–º–æ–∂–Ω—ã–º–∏ –¥–µ—Ñ–∏—Å–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ —Å–ª–æ–≤–∞.

**–ü—Ä–∏–º–µ—Ä—ã:**
```python
tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")              # ‚Üí ["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]
tokenize("hello,world!!!")          # ‚Üí ["hello", "world"]
tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")     # ‚Üí ["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]
tokenize("2025 –≥–æ–¥")                # ‚Üí ["2025", "–≥–æ–¥"]
tokenize("emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ")        # ‚Üí ["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]
```

### 3. `count_freq(tokens: list[str]) -> dict[str, int]`

–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ –≤ —Å–ø–∏—Å–∫–µ —Ç–æ–∫–µ–Ω–æ–≤.

**–ü—Ä–∏–º–µ—Ä—ã:**
```python
count_freq(["a","b","a","c","b","a"])  # ‚Üí {"a": 3, "b": 2, "c": 1}
count_freq(["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä", "–ø—Ä–∏–≤–µ—Ç"])  # ‚Üí {"–ø—Ä–∏–≤–µ—Ç": 2, "–º–∏—Ä": 1}
```

### 4. `top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]`

–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤. –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã, –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ ‚Äî –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É.

**–ü—Ä–∏–º–µ—Ä—ã:**
```python
top_n({"a": 3, "b": 2, "c": 1}, 2)       # ‚Üí [("a", 3), ("b", 2)]
top_n({"bb": 2, "aa": 2, "cc": 1}, 2)    # ‚Üí [("aa", 2), ("bb", 2)]
```

### –ö–æ–¥ –º–æ–¥—É–ª—è `text.py`

```python
import re
from typing import Dict, List, Tuple, Set

def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    result = text

    # –ó–∞–º–µ–Ω–∞ —ë/–Å –Ω–∞ –µ/–ï
    if yo2e:
        result = result.replace('—ë', '–µ').replace('–Å', '–ï')

    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    if casefold:
        result = result.casefold()

    # –ó–∞–º–µ–Ω–∞ —É–ø—Ä–∞–≤–ª—è—é—â–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
    result = result.replace('\t', ' ').replace('\r', ' ').replace('\n', ' ')

    # –°—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –æ–¥–∏–Ω
    result = re.sub(r'\s+', ' ', result)

    # –û–±—Ä–µ–∑–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ —Å –∫—Ä–∞—ë–≤
    result = result.strip()

    return result


def tokenize(text: str) -> List[str]:
    # –®–∞–±–ª–æ–Ω: –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å \w, –¥–æ–ø—É—Å–∫–∞—é—â–∞—è –¥–µ—Ñ–∏—Å—ã –≤–Ω—É—Ç—Ä–∏
    pattern = r'\w+(?:-\w+)*'
    tokens = re.findall(pattern, text)
    return tokens


def count_freq(tokens: List[str]) -> Dict[str, int]:
    freq: Dict[str, int] = {}
    for token in tokens:
        freq[token] = freq.get(token, 0) + 1
    return freq


def top_n(freq: Dict[str, int], n: int = 5) -> List[Tuple[str, int]]:
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–ª—é—á—É: (-—á–∞—Å—Ç–æ—Ç–∞, —Å–ª–æ–≤–æ)
    sorted_items = sorted(freq.items(), key=lambda x: (-x[1], x[0]))
    return sorted_items[:n]


def print_test_cases():
    print("–¢–ï–°–¢-–ö–ï–ô–°–´ –î–õ–Ø –§–£–ù–ö–¶–ò–ô –û–ë–†–ê–ë–û–¢–ö–ò –¢–ï–ö–°–¢–ê")
    
    # –¢–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è normalize
    print("\n1. –§–£–ù–ö–¶–ò–Ø normalize:")
    
    test_cases_normalize = [
        ('"–ü—Ä–ò–≤–ï—Ç\\n–ú–ò—Ä\\t"', 'normalize("–ü—Ä–ò–≤–ï—Ç\\n–ú–ò—Ä\\t")', '"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"', '(casefold + —Å—Ö–ª–æ–ø–Ω—É—Ç—å –ø—Ä–æ–±–µ–ª—ã)'),
        ('"—ë–∂–∏–∫, –Å–ª–∫–∞"', 'normalize("—ë–∂–∏–∫, –Å–ª–∫–∞")', '"–µ–∂–∏–∫, –µ–ª–∫–∞"', '(yo2e=True)'),
        ('"Hello\\r\\nWorld"', 'normalize("Hello\\r\\nWorld")', '"hello world"', ''),
        ('"  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  "', 'normalize("  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ")', '"–¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã"', '')
    ]
    
    for input_desc, func_call, expected, comment in test_cases_normalize:
        print(f"‚Ä¢ –í—Ö–æ–¥: {input_desc}")
        print(f"  –í—ã–∑–æ–≤: {func_call}")
        print(f"  –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {expected}")
        if comment:
            print(f"  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {comment}")
        print()
    
    # –¢–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è tokenize
    print("\n2. –§–£–ù–ö–¶–ò–Ø tokenize:")
    print("-" * 40)
    print("(–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ç–µ–∫—Å—Ç —É–∂–µ normalize)")
    print()
    
    test_cases_tokenize = [
        ('"–ø—Ä–∏–≤–µ—Ç –º–∏—Ä"', 'tokenize("–ø—Ä–∏–≤–µ—Ç –º–∏—Ä")', '["–ø—Ä–∏–≤–µ—Ç", "–º–∏—Ä"]', ''),
        ('"hello,world!!!"', 'tokenize("hello,world!!!")', '["hello", "world"]', ''),
        ('"–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ"', 'tokenize("–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ")', '["–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É", "–∫—Ä—É—Ç–æ"]', ''),
        ('"2025 –≥–æ–¥"', 'tokenize("2025 –≥–æ–¥")', '["2025", "–≥–æ–¥"]', ''),
        ('"–µ–º–æ—ò—ñ üòä –Ω–µ —Å–ª–æ–≤–æ"', 'tokenize("–µ–º–æ—ò—ñ üòä –Ω–µ —Å–ª–æ–≤–æ")', '["emoji", "–Ω–µ", "—Å–ª–æ–≤–æ"]', '(—ç–º–æ–¥–∑–∏ –≤—ã–ø–∞–¥–∞—é—Ç)')
    ]
    
    for input_desc, func_call, expected, comment in test_cases_tokenize:
        print(f"‚Ä¢ –í—Ö–æ–¥: {input_desc}")
        print(f"  –í—ã–∑–æ–≤: {func_call}")
        print(f"  –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {expected}")
        if comment:
            print(f"  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: {comment}")
        print()
    
    # –¢–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è count_freq + top_n
    print("\n3. –§–£–ù–ö–¶–ò–ò count_freq + top_n:")
    
    print("‚Ä¢ –¢–µ—Å—Ç 1:")
    print("  –í—Ö–æ–¥ (—Ç–æ–∫–µ–Ω—ã): [\"a\",\"b\",\"a\",\"c\",\"b\",\"a\"]")
    print("  –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—á–∞—Å—Ç–æ—Ç—ã): {\"a\":3,\"b\":2,\"c\":1}")
    print("  –í—ã–∑–æ–≤: top_n(..., n=2)")
    print("  –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: [(\"a\",3), (\"b\",2)]")
    print()
    
    print("‚Ä¢ –¢–µ—Å—Ç 2 (—Ç–∞–π-–±—Ä–µ–π–∫):")
    print("  –í—Ö–æ–¥ (—Ç–æ–∫–µ–Ω—ã): [\"bb\",\"aa\",\"bb\",\"aa\",\"cc\"]")
    print("  –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—á–∞—Å—Ç–æ—Ç—ã): {\"aa\":2,\"bb\":2,\"cc\":1}")
    print("  –í—ã–∑–æ–≤: top_n(..., n=2)")
    print("  –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: [(\"aa\",2), (\"bb\",2)]")
    print("  –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: (–∞–ª—Ñ–∞–≤–∏—Ç–Ω–∞—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ)")
    print()

    print("–ö–û–ù–ï–¶ –¢–ï–°–¢-–ö–ï–ô–°–û–í")


def demo():
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –†–ê–ë–û–¢–´ –§–£–ù–ö–¶–ò–ô")
    
    # –ü—Ä–∏–º–µ—Ä 1: –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏
    print("\n–ü—Ä–∏–º–µ—Ä 1: –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞")
    text = "–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä! –≠—Ç–æ —Ç–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç —Å —á–∏—Å–ª–∞–º–∏ 2025."
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {repr(text)}")
    
    normalized = normalize(text)
    print(f"–ü–æ—Å–ª–µ normalize: {repr(normalized)}")
    
    tokens = tokenize(normalized)
    print(f"–ü–æ—Å–ª–µ tokenize: {tokens}")
    
    freq = count_freq(tokens)
    print(f"–ß–∞—Å—Ç–æ—Ç—ã: {freq}")
    
    top_words = top_n(freq, 3)
    print(f"–¢–æ–ø-3 —Å–ª–æ–≤–∞: {top_words}")
    
    # –ü—Ä–∏–º–µ—Ä 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —ç–º–æ–¥–∑–∏
    print("\n–ü—Ä–∏–º–µ—Ä 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —ç–º–æ–¥–∑–∏")
    text2 = "Python üòä —ç—Ç–æ –∫—Ä—É—Ç–æ! Python –æ—á–µ–Ω—å –º–æ—â–Ω—ã–π üöÄ"
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {repr(text2)}")
    
    normalized2 = normalize(text2)
    tokens2 = tokenize(normalized2)
    freq2 = count_freq(tokens2)
    top_words2 = top_n(freq2, 2)
    
    print(f"–¢–æ–∫–µ–Ω—ã: {tokens2}")
    print(f"–¢–æ–ø-2 —Å–ª–æ–≤–∞: {top_words2}")


if __name__ == "__main__":
    import sys
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    if len(sys.argv) > 1:
        if sys.argv[1] == "--demo":
            # –†–µ–∂–∏–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            print_test_cases()
            demo()
            sys.exit(0)
        elif sys.argv[1] == "--no-cases":
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–≤–æ–¥ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤
            skip_cases = True
        else:
            print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
            print("  python text.py           - –∑–∞–ø—É—Å–∫ —Å —Ç–µ—Å—Ç-–∫–µ–π—Å–∞–º–∏ –∏ —Ç–µ—Å—Ç–∞–º–∏")
            print("  python text.py --demo    - –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π")
            print("  python text.py --no-cases - —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç—ã –±–µ–∑ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤")
            sys.exit(1)
    else:
        skip_cases = False
        # –í—ã–≤–æ–¥–∏–º —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –ø—Ä–∏ –∫–∞–∂–¥–æ–º –∑–∞–ø—É—Å–∫–µ
        print_test_cases()
```

## –ó–∞–¥–∞–Ω–∏–µ B ‚Äî –°–∫—Ä–∏–ø—Ç `src/lab03/text_stats.py`
![–¢–µ—Å—Ç text_stats.py](../../img/lab03/B_1.png)
![–¢–µ—Å—Ç text_stats.py](../../img/lab03/B_1+.png)
–°–∫—Ä–∏–ø—Ç —á–∏—Ç–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ `stdin`, –≤—ã–∑—ã–≤–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –º–æ–¥—É–ª—è `text.py` –∏ –≤—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É:
- –í—Å–µ–≥–æ —Å–ª–æ–≤
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤
- –¢–æ–ø-5 —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö —Å–ª–æ–≤

### –ö–æ–¥ —Å–∫—Ä–∏–ø—Ç–∞ `text_stats.py`

```python
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ –∏–∑ stdin –∏ –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    echo "–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞" | python text_stats.py
    cat file.txt | python text_stats.py

–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
    TABLE_FORMAT=1 - –≤–∫–ª—é—á–∏—Ç—å —Ç–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—ã–≤–æ–¥–∞
"""

import sys
import os
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—é lib –≤ sys.path
lib_path = Path(__file__).parent.parent / 'lib'
sys.path.insert(0, str(lib_path))

from text import normalize, tokenize, count_freq, top_n


def print_stats(text: str, top_count: int = 5, table_format: bool = False) -> None:
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    normalized = normalize(text)
    tokens = tokenize(normalized)
    
    # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç
    freq = count_freq(tokens)
    top_words = top_n(freq, top_count)
    
    # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print(f"–í—Å–µ–≥–æ —Å–ª–æ–≤: {len(tokens)}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: {len(freq)}")
    
    if table_format and top_words:
        print_table(top_words)
    else:
        print_simple(top_words)


def print_simple(top_words: list) -> None:
    print(f"–¢–æ–ø-{len(top_words)}:")
    for word, freq in top_words:
        print(f"{word}:{freq}")


def print_table(top_words: list) -> None:
    if not top_words:
        return
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–ª–æ–≤
    max_word_len = max(len(word) for word, _ in top_words)
    word_width = max(max_word_len, len("—Å–ª–æ–≤–æ"))
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∏—Ä–∏–Ω—É –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —á–∞—Å—Ç–æ—Ç
    max_freq_len = max(len(str(freq)) for _, freq in top_words)
    freq_width = max(max_freq_len, len("—á–∞—Å—Ç–æ—Ç–∞"))
    
    # –í—ã–≤–æ–¥–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    print(f"\n–¢–æ–ø-{len(top_words)}:")
    print(f"{'—Å–ª–æ–≤–æ':<{word_width}} | {'—á–∞—Å—Ç–æ—Ç–∞':<{freq_width}}")
    print("-" * (word_width + freq_width + 3))
    
    # –í—ã–≤–æ–¥–∏–º –¥–∞–Ω–Ω—ã–µ
    for word, freq in top_words:
        print(f"{word:<{word_width}} | {freq:<{freq_width}}")


def main():
    # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å –≤–≤–æ–¥ –∏–∑ stdin
    try:
        text = sys.stdin.read()
    except KeyboardInterrupt:
        print("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º", file=sys.stderr)
        sys.exit(1)
    
    if not text.strip():
        print("–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π –≤–≤–æ–¥", file=sys.stderr)
        sys.exit(1)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤–∫–ª—é—á–µ–Ω –ª–∏ —Ç–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
    table_format = os.environ.get('TABLE_FORMAT', '0') == '1'
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print_stats(text, top_count=5, table_format=table_format)


if __name__ == "__main__":
    main()
```

## –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### –ü—Ä–∏–º–µ—Ä 1: –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç

**–í–≤–æ–¥:**
```bash
echo "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç!!!" | python3 src/lab03/text_stats.py
```

**–í—ã–≤–æ–¥:**
```
–í—Å–µ–≥–æ —Å–ª–æ–≤: 3
–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 2
–¢–æ–ø-2:
–ø—Ä–∏–≤–µ—Ç:2
–º–∏—Ä:1
```

### –ü—Ä–∏–º–µ—Ä 2: –¢–µ–∫—Å—Ç —Å —ë –∏ –¥–µ—Ñ–∏—Å–∞–º–∏

**–í–≤–æ–¥:**
```bash
echo "–Å–∂–∏–∫ –∏ –µ–∂–∏–∫ –≥—É–ª—è–ª–∏ –ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –≤–µ—Å–µ–ª–æ. –Å–ª–∫–∞ –∫—Ä–∞—Å–∏–≤–∞!" | python3 src/lab03/text_stats.py
```

**–í—ã–≤–æ–¥:**
```
–í—Å–µ–≥–æ —Å–ª–æ–≤: 8
–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 7
–¢–æ–ø-5:
–µ–∂–∏–∫:2
–≤–µ—Å–µ–ª–æ:1
–≥—É–ª—è–ª–∏:1
–µ–ª–∫–∞:1
–∏:1
```

**–ü–æ—è—Å–Ω–µ–Ω–∏–µ:** –°–ª–æ–≤–∞ "–Å–∂–∏–∫" –∏ "–µ–∂–∏–∫" –±—ã–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ –æ–¥–Ω–æ —Å–ª–æ–≤–æ "–µ–∂–∏–∫", —Ç–∞–∫ –∫–∞–∫ `—ë` –∑–∞–º–µ–Ω—è–µ—Ç—Å—è –Ω–∞ `–µ`.

### –ü—Ä–∏–º–µ—Ä 3: –¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç

**–í–≤–æ–¥:**
```bash
echo "–ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! –ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞? –ú–∏—Ä –ø—Ä–µ–∫—Ä–∞—Å–µ–Ω, –∫–æ–≥–¥–∞ –≤—Å–µ —Ö–æ—Ä–æ—à–æ. –ü—Ä–∏–≤–µ—Ç –≤—Å–µ–º!" | TABLE_FORMAT=1 python3 src/lab03/text_stats.py
```

**–í—ã–≤–æ–¥:**
```
–í—Å–µ–≥–æ —Å–ª–æ–≤: 12
–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 9

–¢–æ–ø-5:
—Å–ª–æ–≤–æ  | —á–∞—Å—Ç–æ—Ç–∞
----------------
–ø—Ä–∏–≤–µ—Ç | 3      
–º–∏—Ä    | 2      
–≤—Å–µ    | 1      
–≤—Å–µ–º   | 1      
–¥–µ–ª–∞   | 1      
```

### –ü—Ä–∏–º–µ—Ä 4: –¢–µ–∫—Å—Ç —Å —É–ø—Ä–∞–≤–ª—è—é—â–∏–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏

**–í–≤–æ–¥:**
```bash
echo -e "Hello\tWorld\n–ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ\r\n–Ω–∞ Python" | python3 src/lab03/text_stats.py
```

**–í—ã–≤–æ–¥:**
```
–í—Å–µ–≥–æ —Å–ª–æ–≤: 5
–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤: 5
–¢–æ–ø-5:
hello:1
world:1
–Ω–∞:1
–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ:1
python:1
```

## –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏

### –ó–∞–ø—É—Å–∫ –º–æ–¥—É–ª—è text.py

```bash
cd src/lib
python3 text.py
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –í—ã–≤–æ–¥—è—Ç—Å—è —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –¥–ª—è –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π.

### –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Ñ—É–Ω–∫—Ü–∏–π

```bash
cd src/lib
python3 text.py --demo
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –í—ã–≤–æ–¥—è—Ç—Å—è —Ç–µ—Å—Ç-–∫–µ–π—Å—ã –∏ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö.

### –ó–∞–ø—É—Å–∫ –±–µ–∑ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤

```bash
cd src/lib
python3 text.py --no-cases
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:** –ó–∞–ø—É—Å–∫–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç—ã –±–µ–∑ –≤—ã–≤–æ–¥–∞ —Ç–µ—Å—Ç-–∫–µ–π—Å–æ–≤.

## –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞:**
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è `casefold()` –≤–º–µ—Å—Ç–æ `lower()` –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å Unicode
   - –°–∏–º–≤–æ–ª—ã `—ë/–Å` –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ `–µ/–ï`
   - –£–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã (`\t`, `\r`, `\n`) –∑–∞–º–µ–Ω—è—é—Ç—Å—è –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
   - –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã —Å—Ö–ª–æ–ø—ã–≤–∞—é—Ç—Å—è –≤ –æ–¥–∏–Ω

2. **–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è:**
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ `\w+(?:-\w+)*`
   - –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Å–ª–æ–≤–∞ —Å –¥–µ—Ñ–∏—Å–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É")
   - –ß–∏—Å–ª–∞ —Å—á–∏—Ç–∞—é—Ç—Å—è —Ç–æ–∫–µ–Ω–∞–º–∏
   - –≠–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è

3. **–ü–æ–¥—Å—á—ë—Ç —á–∞—Å—Ç–æ—Ç:**
   - –ü—Ä–æ—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Å—Ç–æ—Ç
   - –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å O(n) –¥–ª—è –ø–æ–¥—Å—á—ë—Ç–∞

4. **–¢–æ–ø-N:**
   - –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —á–∞—Å—Ç–æ—Ç—ã
   - –ü—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ —á–∞—Å—Ç–æ—Ç ‚Äî –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É (–≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏–µ)
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª—é—á —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ `(-—á–∞—Å—Ç–æ—Ç–∞, —Å–ª–æ–≤–æ)`

5. **–¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç:**
   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ —Å—Ç–æ–ª–±—Ü–æ–≤
   - –í–∫–ª—é—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `TABLE_FORMAT=1`
   - –ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Å —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏

## –¢–∏–ø–æ–≤—ã–µ –æ—à–∏–±–∫–∏ –∏ –∏—Ö —Ä–µ—à–µ–Ω–∏—è

1. **`—ë` vs `–µ`:**
   - –ü—Ä–æ–±–ª–µ–º–∞: "—ë–∂–∏–∫" –∏ "–µ–∂–∏–∫" —Å—á–∏—Ç–∞—é—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
   - –†–µ—à–µ–Ω–∏–µ: –∑–∞–º–µ–Ω–∞ `—ë‚Üí–µ` –≤ —Ñ—É–Ω–∫—Ü–∏–∏ `normalize()`

2. **`lower()` vs `casefold()`:**
   - –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ Unicode-—Å–∏–º–≤–æ–ª–∞–º–∏
   - –†–µ—à–µ–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ `casefold()` –≤–º–µ—Å—Ç–æ `lower()`

3. **–î–µ—Ñ–∏—Å—ã –≤ —Å–ª–æ–≤–∞—Ö:**
   - –ü—Ä–æ–±–ª–µ–º–∞: "–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É" —Ä–∞–∑–±–∏–≤–∞–µ—Ç—Å—è –Ω–∞ –¥–≤–∞ —Ç–æ–∫–µ–Ω–∞
   - –†–µ—à–µ–Ω–∏–µ: —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ `\w+(?:-\w+)*`

4. **–î–ª–∏–Ω–Ω–æ–µ —Ç–∏—Ä–µ:**
   - –ü—Ä–æ–±–ª–µ–º–∞: "—Å–ª–æ–≤–æ‚Äî—Å–ª–æ–≤–æ" –Ω–µ —Ä–∞–∑–¥–µ–ª—è–µ—Ç—Å—è
   - –†–µ—à–µ–Ω–∏–µ: –¥–ª–∏–Ω–Ω–æ–µ —Ç–∏—Ä–µ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–∏–º–≤–æ–ª–æ–º `\w`, –ø–æ—ç—Ç–æ–º—É –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–ª—É–∂–∏—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–º

